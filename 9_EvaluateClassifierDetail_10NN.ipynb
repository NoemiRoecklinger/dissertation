{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN size \n",
    "NN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in all data\n",
    "\n",
    "After calculating the overall accuracy, I will not work with the training dataset anymore, but only test it on the testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploadng my datasets\n",
    "# Define a data frame with all my data# Define  \n",
    "FILE_PATH = r\"../DATA/ML_datasets/Initial_setup\"\n",
    "META_FILE_PATH = \"../DATA/META\"\n",
    "IMAGE_FILE_PATH = r\"images\"\n",
    "\n",
    "# Testing data\n",
    "# We do not have to sownsample the testing data\n",
    "y_test = np.loadtxt(FILE_PATH+'/y_test_'+ str(NN) +'NN.txt', delimiter=',')\n",
    "X_test = np.loadtxt(FILE_PATH+'/X_test_'+ str(NN) +'NN.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For RF classifier: cast as float32 \n",
    "y_test = y_test.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.loadtxt(FILE_PATH+'/y_train_'+ str(NN) +'NN.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###uploading the saved classifier\n",
    "filename = '/randomforest_model_'+ str(NN) +'NN.sav'\n",
    "clf = joblib.load(FILE_PATH + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictions\n",
    "predictions_train = np.loadtxt(FILE_PATH+'/predictions_train_'+ str(NN) +'NN.txt', delimiter=',')\n",
    "predictions_test = np.loadtxt(FILE_PATH+'/predictions_test_'+ str(NN) +'NN.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now start evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show problem with threshold or unclassified\n",
    "\n",
    "I thought I could determine a threshold or a difference between two values to create a class unclassified. But this will not work. Especially the predictions for the noisy and small classes are really bad. They are never the highest in the probability array. Mostly, they get classified as building. Even if I set a threshold, the classsifier is still sure that it must be a building. Also, there is a biggap to the next highest value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all probabilities for the testing set \n",
    "probability_array = clf.predict_proba(X_test)\n",
    "print \"This is the shape of the array\", probability_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max 90.59667185706162 %\n",
      "sec Max 4.1939604148964955 %\n",
      "Difference between highest and second highest value 86.40271144216513 %\n"
     ]
    }
   ],
   "source": [
    "probability_array.sort()\n",
    "print \"Max\", (probability_array[0][-1])*100, \"%\"\n",
    "print \"sec Max\", (probability_array[0][-2])*100, \"%\"\n",
    "print \"Difference between highest and second highest value\", (probability_array[0][-1])*100 - (probability_array[0][-2])*100, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 646126,  176841,   11971, 1271960,     768,    3605,    4614,\n",
       "           7960,   63779,   22835]),\n",
       " array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of instances per class in testing set  \n",
    "np.histogram(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look where the value should be class 11 (potted plant)\n",
    "np.where(y_test == 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.27756543e-04, 3.30485854e-03, 4.92809204e-03, 4.95491760e-03,\n",
       "       9.05750052e-03, 9.53493209e-03, 1.51425102e-02, 1.85366707e-02,\n",
       "       2.27663613e-02, 1.23530836e-01, 7.87615564e-01])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For these values, check the probability \n",
    "probability_array[2209993]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. value for class 1: 0.3699568958628669 %\n",
      "Max. value for class 2: 0.6751852446548976 %\n",
      "Max. value for class 3: 1.0076837122626885 %\n",
      "Max. value for class 4: 1.7199943004976337 %\n",
      "Max. value for class 5: 2.051003130842827 %\n",
      "Max. value for class 6: 5.377239347152576 %\n",
      "Max. value for class 7: 11.36167740470613 %\n",
      "Max. value for class 8: 22.72579506396993 %\n",
      "Max. value for class 9: 31.249614158660265 %\n",
      "Max. value for class 10: 49.00685656977799 %\n",
      "Max. value for class 11: 96.53842574048637 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,11):\n",
    "    print \"Max. value for class \" +str(i+1)+ \":\", probability_array[:,i].max()*100, \"%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2210459,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_array[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9171416434324274"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing\n",
    "f1 = f1_score(y_test, predictions_test, average=\"weighted\")\n",
    "rec = recall_score(y_test, predictions_test, average=\"weighted\")\n",
    "prec = precision_score(y_test, predictions_test, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training\n",
    "f1_train = f1_score(y_train, predictions_train, average=\"macro\")\n",
    "rec_train = recall_score(y_train, predictions_train, average=\"macro\")\n",
    "prec_train = precision_score(y_train, predictions_train, average=\"macro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.893543640791804\n",
      "recall 0.9171416434324274\n",
      "precission 0.8949788778688254\n"
     ]
    }
   ],
   "source": [
    "print \"F1\", f1\n",
    "print \"recall\", rec\n",
    "print \"precission\", prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.2532961984983418\n",
      "recall 0.2510855137093803\n",
      "precission 0.32834623914232464\n"
     ]
    }
   ],
   "source": [
    "print \"F1\", f1_train\n",
    "print \"recall\", rec_train\n",
    "print \"precission\", prec_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'                 precision    recall  f1-score   support\\n\\n           Road       0.93      0.97      0.95    646126\\n       Sidewalk       0.83      0.75      0.79    176841\\n           Curb       0.00      0.00      0.00     11971\\n       Building       0.92      1.00      0.96   1271960\\nOther pole-like       0.00      0.00      0.00       768\\n    Small poles       0.00      0.00      0.00      3605\\n    Pedestrians       0.00      0.00      0.00      4614\\n     2-wheelers       0.00      0.00      0.00      7960\\n     4-wheelers       0.93      0.05      0.09     63779\\n          trees       0.00      0.00      0.00     21041\\n  Potted plants       0.00      0.00      0.00      1794\\n\\n    avg / total       0.89      0.92      0.89   2210459\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the evaluation file\n",
    "target_names = ['Road', 'Sidewalk', 'Curb', 'Building', 'Other pole-like', \n",
    "                'Small poles', 'Pedestrians', '2-wheelers', '4-wheelers','trees', 'Potted plants']\n",
    "\n",
    "classification_report(y_test, predictions_test, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'                 precision    recall  f1-score   support\\n\\n           Road       0.93      0.97      0.95    646126\\n       Sidewalk       0.83      0.75      0.79    176841\\n           Curb       0.00      0.00      0.00     11971\\n       Building       0.92      1.00      0.96   1271960\\nOther pole-like       0.00      0.00      0.00       768\\n    Small poles       0.00      0.00      0.00      3605\\n    Pedestrians       0.00      0.00      0.00      4614\\n     2-wheelers       0.00      0.00      0.00      7960\\n     4-wheelers       0.93      0.05      0.09     63779\\n          trees       0.00      0.00      0.00     21041\\n  Potted plants       0.00      0.00      0.00      1794\\n\\n    avg / total       0.89      0.92      0.89   2210459\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_report_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix  = confusion_matrix(y_test, predictions_test)\n",
    "\n",
    "np.savetxt(FILE_PATH + \"/confusionMatrix_scenarioA1.txt\", conf_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "           Road       0.93      0.97      0.95    646126\n",
      "       Sidewalk       0.83      0.75      0.79    176841\n",
      "           Curb       0.00      0.00      0.00     11971\n",
      "       Building       0.92      1.00      0.96   1271960\n",
      "Other pole-like       0.00      0.00      0.00       768\n",
      "    Small poles       0.00      0.00      0.00      3605\n",
      "    Pedestrians       0.00      0.00      0.00      4614\n",
      "     2-wheelers       0.00      0.00      0.00      7960\n",
      "     4-wheelers       0.93      0.05      0.09     63779\n",
      "          trees       0.00      0.00      0.00     21041\n",
      "  Potted plants       0.00      0.00      0.00      1794\n",
      "\n",
      "    avg / total       0.89      0.92      0.89   2210459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the evaluation file\n",
    "target_names = ['Road', 'Sidewalk', 'Curb', 'Building', 'Other pole-like', \n",
    "                'Small poles', 'Pedestrians', '2-wheelers', '4-wheelers','trees', 'Potted plants']\n",
    "\n",
    "clf_report_A1 = classification_report(y_test, predictions_test, target_names=target_names)\n",
    "clf_report_A1 = clf_report_A1.encode('ascii','ignore')\n",
    "print clf_report_A1\n",
    "\n",
    "# Save it in new text file\n",
    "file = open(FILE_PATH + \"/clf_report_scenarioA1.txt\", \"w\")\n",
    "file.write(clf_report_A1)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../DATA/ML_datasets/Initial_setup\n"
     ]
    }
   ],
   "source": [
    "print FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
