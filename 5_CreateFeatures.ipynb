{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is my file to create all features for the ML process\n",
    "\n",
    "I have my cleaned file with revised classes (1-11): Dataset_for_ML \n",
    "\n",
    "Now I take that and calculate all the other features for it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import scipy.linalg as scplinag\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCovarianceMatrix(data):\n",
    "    \"\"\"\n",
    "    Function to compute the covariance matrix.\n",
    "    \n",
    "    Input: Dataset of 3D points; i.e. array of dimension: #points x 3 \n",
    "    Output: 3x3 covariance matrix (np.array)\n",
    "    \"\"\"\n",
    "    # Create covariance matrix and array to store the mean values for x_mean, y_mean, z_mean\n",
    "    C = np.zeros((data.shape[1], data.shape[1]))\n",
    "    mean_xyz = []\n",
    "    # Calculate all mean values\n",
    "    for i in range(0, data.shape[1]):\n",
    "        mean_xyz.append(data[:,i].mean())\n",
    "    mean_xyz = np.array(mean_xyz)\n",
    "    # Check whether dimensions agree \n",
    "    if data[:,0].size != data[:,1].size or data[:,0].size != data[:,2].size:\n",
    "        print \"X, Y and Z must be of same dimensions.\"\n",
    "    else:\n",
    "        # For each row in covariance matrix C\n",
    "        for i in range(0, C.shape[0]):\n",
    "            # For each column in covariance matrix C\n",
    "            for j in range(0, C.shape[1]):\n",
    "                C[i,j] = 0\n",
    "                # For each point in the dataset, access x, y, z-values\n",
    "                for point in data:\n",
    "                    # For each point, access x,y and z in all combinations (xx, xy, xz, yx, yy, yz etc)\n",
    "                    C[i][j] = C[i][j] + (point[i]-mean_xyz[i])*(point[j]-mean_xyz[j])\n",
    "    # Divide by the total number of points                \n",
    "    C = (1.0/data.shape[0]) * C\n",
    "    return C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get eight parameters for each point\n",
    "\n",
    "def calcFeatureDescr(covarianceMatrix):\n",
    "    \"\"\"\n",
    "    Function to compute the 8 feature descriptors for each point.\n",
    "    \n",
    "    Input: 3x3 Covariance matrix of a point and its neighbourhood \n",
    "    \n",
    "    Output: np Array with feature descriptors as described by Weinmann et al. (1D array with 8 elements)\n",
    "    \n",
    "    \"\"\"\n",
    "    D, V = scplinag.eigh(covarianceMatrix)\n",
    "    # We sort the array with eigenvalues by size (from smallest to largest value)\n",
    "    D.sort()\n",
    "    # Get eigenvectors\n",
    "    e1 = V[2] # eigenvector in direction of largest variance\n",
    "    e2 = V[1] # second eigenvector, perpend. to e1\n",
    "    e3 = V[0]\n",
    "    # Find the eigenvalues\n",
    "    evalue1 = D[2] # largest\n",
    "    evalue2 = D[1]\n",
    "    evalue3 = D[0] # smallest\n",
    "\n",
    "    # Linearity\n",
    "    lambda1 = (evalue1 - evalue2) / evalue1\n",
    "    # Planarity\n",
    "    lambda2 = (evalue2 - evalue3) / evalue1\n",
    "    # Scattering\n",
    "    lambda3 = evalue3 / evalue1\n",
    "    # Omnivariance\n",
    "    lambda4 = pow(evalue1*evalue2*evalue3, 1/3.0)\n",
    "    # Anisotropy\n",
    "    lambda5 = (evalue1 - evalue3) / evalue1\n",
    "    # Eigentropy\n",
    "    s = 0\n",
    "    for elem in D:\n",
    "        s = s + (elem*np.log(elem))\n",
    "    lambda6 = (-1)*s  \n",
    "    # Sum of eigenvalues\n",
    "    lambda7 = sum(D)\n",
    "    # Change of curvature\n",
    "    lambda8 = evalue3/sum(D) \n",
    "    \n",
    "    featureDescriptor = np.array([lambda1, lambda2, lambda3, lambda4, lambda5, lambda6, lambda7, lambda8])\n",
    "    return featureDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data frame with all my data# Define  \n",
    "FILE_PATH = r\"../DATA\"\n",
    "FILE_NAME = r\"/Dataset_for_ML_verticality.txt\"\n",
    "IMAGE_FILE_PATH = r\"images\"\n",
    "df = pd.read_csv(FILE_PATH+FILE_NAME, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to NumPy array \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 11052294\n",
      "Number of columns 6\n"
     ]
    }
   ],
   "source": [
    "# Data is the whole dataset but as a numpy array \n",
    "data = df.values\n",
    "rows, columns = data.shape\n",
    "print \"Number of rows:\", rows\n",
    "print \"Number of columns\", columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only XYZ values\n",
    "dataxyz = data[:,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute all features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all points now \n",
    "# Create kd-tree\n",
    "kdt = KDTree(dataxyz, leaf_size=40, metric='euclidean')\n",
    "# Get list with indices, the first value is always the point itself\n",
    "idx_list = kdt.query(dataxyz, k=100, return_distance=False)\n",
    "store = []\n",
    "for j in range(0, dataxyz.shape[0]):\n",
    "    # Look at all points now\n",
    "    neighbourhood = []\n",
    "    for i in idx_list[j]:\n",
    "        neighbourhood.append(dataxyz[i])\n",
    "    neighbourhood = np.array(neighbourhood)\n",
    "    # Everything we did before with dataset, we do now with the neighbourhood only\n",
    "    C = calcCovarianceMatrix(neighbourhood)\n",
    "    feat = calcFeatureDescr(C)\n",
    "    row_with_additional_col = np.append(dataxyz[j], feat)\n",
    "    store.append(row_with_additional_col)\n",
    "store = np.array(store)\n",
    "print \"This is the shape of the file:\", store.shape\n",
    "\n",
    "\n",
    "# Create a data frame with the calculated features \n",
    "df2 = pd.DataFrame({\n",
    "    'X': store[:,0],\n",
    "    'Y': store[:,1],\n",
    "    'Z': store[:,2],\n",
    "    'range': df['relative_height'],\n",
    "    'verticality': df['verticality'],\n",
    "    'lambda1': store[:,3],\n",
    "    'lambda2': store[:,4],\n",
    "    'lambda3': store[:,5],\n",
    "    'lambda4': store[:,6],\n",
    "    'lambda5': store[:,7],\n",
    "    'lambda6': store[:,8],\n",
    "    'lambda7': store[:,9],\n",
    "    'lambda8': store[:,10],\n",
    "    'class': df['class']\n",
    "})\n",
    "df2.to_csv(FILE_PATH+'/5_Data_ML_attributes_100NN.txt', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
