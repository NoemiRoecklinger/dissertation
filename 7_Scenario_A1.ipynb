{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario A.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data frame with all my data# Define  \n",
    "FILE_PATH = r\"../DATA/ML_datasets/Initial_setup\"\n",
    "META_FILE_PATH = \"../DATA/META\"\n",
    "IMAGE_FILE_PATH = r\"images\"\n",
    "\n",
    "y_train = np.loadtxt(FILE_PATH+'/y_train_xxNN.txt', delimiter=',')\n",
    "X_train = np.loadtxt(FILE_PATH+'/X_train_xxNN.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For RF classifier \n",
    "y_train = y_train.astype('float32')\n",
    "X_train = X_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Random Forest in: 0.000479936599731 seconds\n",
      "Fit model in: 1.35925292969 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf = RandomForestClassifier(max_depth=4, random_state=42, n_estimators=100, criterion='gini')\n",
    "print 'Created Random Forest in:', float(time.time()-start), 'seconds'\n",
    "start = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print 'Fit model in:', float(time.time()-start), 'seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now make predictions\n",
    "\n",
    "Predict and look at probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = X_train[:4]\n",
    "test_predictions_labels = y_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: [4.]\n",
      "True label: 4.0\n",
      "Predicted with  [[2.22905211e-03 5.11543282e-03 4.79699423e-04 9.48947408e-01\n",
      "  4.26735393e-04 6.82916556e-04 3.05624028e-03 4.63174084e-03\n",
      "  2.46972028e-02 8.87533872e-03 8.58233154e-04]] probability\n",
      "Predicted value: [10.]\n",
      "True label: 10.0\n",
      "Predicted with  [[1.20419129e-04 5.58073894e-04 2.58525088e-05 3.31276078e-01\n",
      "  4.85370079e-05 5.13220521e-04 1.17557605e-03 1.32692700e-03\n",
      "  2.08854547e-02 6.43640876e-01 4.28985113e-04]] probability\n",
      "Predicted value: [4.]\n",
      "True label: 4.0\n",
      "Predicted with  [[1.73423431e-02 4.13939519e-02 5.69384530e-03 8.04953613e-01\n",
      "  2.14288737e-04 2.27081850e-03 4.39430616e-03 1.73064918e-02\n",
      "  9.83654257e-02 5.85847947e-03 2.20643650e-03]] probability\n",
      "Predicted value: [1.]\n",
      "True label: 1.0\n",
      "Predicted with  [[9.71708678e-01 1.71062240e-02 8.91516794e-04 9.73857582e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.73134328e-06\n",
      "  5.51274317e-04 0.00000000e+00 0.00000000e+00]] probability\n"
     ]
    }
   ],
   "source": [
    "for count, elem in enumerate(test_predictions):\n",
    "    print \"Predicted value:\", clf.predict([elem])\n",
    "    print \"True label:\", test_predictions_labels[count]\n",
    "    print \"Predicted with \", clf.predict_proba([elem]), \"probability\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Numpy array with all predictions for the whole dataset\n",
    "y_predictions = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "From here on, evaluate the classifier we just created. For scenario A.1 all methods were tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.9252\n"
     ]
    }
   ],
   "source": [
    "# Mean accuracy of the dataset \n",
    "# average accuracy is the average of each accuracy per class \n",
    "# (sum of accuracy for each class predicted/number of class)\n",
    "print \"Mean accuracy:\", clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9252\n",
      "Accuracy score (not normalized) 9252\n"
     ]
    }
   ],
   "source": [
    "# Determine accuracy score\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print \"Accuracy score\", accuracy_score(y_train, y_predictions)\n",
    "\n",
    "# This gives me the ttal number of instances that were classified correctly \n",
    "# If I divide that by the number of all instances, I get the first result\n",
    "# return the number of correctly classified samples\n",
    "print \"Accuracy score (not normalized)\", accuracy_score(y_train, y_predictions,normalize=False)\n",
    "\n",
    "# Of course that accuracy will be high because there are a lot of instances for building and road, so when \n",
    "# we are looking at all instances (not single classes), we expect a good result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.93      0.98      0.95      2977\n",
      "        2.0       0.89      0.74      0.81       814\n",
      "        3.0       0.88      0.13      0.23        54\n",
      "        4.0       0.93      1.00      0.96      5687\n",
      "        5.0       0.00      0.00      0.00         2\n",
      "        6.0       0.00      0.00      0.00        11\n",
      "        7.0       0.00      0.00      0.00        19\n",
      "        8.0       0.00      0.00      0.00        39\n",
      "        9.0       0.00      0.00      0.00       286\n",
      "       10.0       0.90      0.51      0.65       105\n",
      "       11.0       0.00      0.00      0.00         6\n",
      "\n",
      "avg / total       0.89      0.93      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "clf_report = classification_report(y_train_subset, y_predictions)\n",
    "clf_report = clf_report.encode('ascii','ignore')\n",
    "print clf_report\n",
    "\n",
    "# Save it in new text file\n",
    "file = open(META_FILE_PATH + \"/clf_report_100NN.txt\", \"w\")\n",
    "file.write(clf_report)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 score: 0.9044\n",
      "Macro F1 score: 0.3277\n",
      "Micro F1 score: 0.9252\n"
     ]
    }
   ],
   "source": [
    "# Compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print \"Weighted F1 score:\", \"%.4f\" % f1_score(y_train_subset, y_predictions, average='weighted')\n",
    "print \"Macro F1 score:\", \"%.4f\" % f1_score(y_train_subset, y_predictions, average='macro')\n",
    "print \"Micro F1 score:\", f1_score(y_train_subset, y_predictions, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro RECALL score: , 0.3056\n",
      "Micro RECALL score: , 0.9252\n",
      "Weighted RECALL score: 0.9252\n",
      "RECALL for each class with average = None [0.9768223  0.74201474 0.12962963 0.99859328 0.         0.\n",
      " 0.         0.         0.         0.51428571 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Compute recall\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "print \"Macro RECALL score:\", \", %.4f\" % recall_score(y_train_subset, y_predictions, average='macro')\n",
    "print \"Micro RECALL score:\", \", %.4f\" % recall_score(y_train_subset, y_predictions, average='micro')\n",
    "print \"Weighted RECALL score:\", \"%.4f\" % recall_score(y_train_subset, y_predictions, average='weighted')\n",
    "print \"RECALL for each class with average = None\", recall_score(y_train_subset, y_predictions, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score (Macro): 0.4112\n",
      "Precision score (Micro): 0.9252\n",
      "Precision score (Weighted): 0.8907\n"
     ]
    }
   ],
   "source": [
    "# Compute precision\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print \"Precision score (Macro):\", \"%.4f\" % precision_score(y_train_subset, y_predictions, average='macro') \n",
    "print \"Precision score (Micro):\", \"%.4f\" % precision_score(y_train_subset, y_predictions, average='micro')\n",
    "print \"Precision score (Weighted):\", \"%.4f\" % precision_score(y_train_subset, y_predictions, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision-Recall-Fscore support (Macro): (0.4112079725268371, 0.3055768793808435, 0.32768384799010464, None)\n",
      "Precision-Recall-Fscore support (Micro): (0.9252, 0.9252, 0.9252, None)\n",
      "Precision-Recall-Fscore support (Weighted): (0.8906690897431508, 0.9252, 0.9044016865923302, None)\n"
     ]
    }
   ],
   "source": [
    "# Precision, recall, F1 and support \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print \"Precision-Recall-Fscore support (Macro):\", precision_recall_fscore_support(y_train_subset, y_predictions, average='macro')\n",
    "print \"Precision-Recall-Fscore support (Micro):\", precision_recall_fscore_support(y_train_subset, y_predictions, average='micro')\n",
    "print \"Precision-Recall-Fscore support (Weighted):\", precision_recall_fscore_support(y_train_subset, y_predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE IMPORTANCE:\n",
      "Linearity 0.0015\n",
      "Planarity 0.0018\n",
      "Scattering 0.0324\n",
      "Omnivariance 0.0773\n",
      "Anisotropy 0.0324\n",
      "Eigentropy 0.0327\n",
      "Sum of eigenvalues 0.0334\n",
      "Change of curvature 0.0230\n",
      "Local density 0.0711\n",
      "Relative height 0.4667\n",
      "Verticality 0.2276\n"
     ]
    }
   ],
   "source": [
    "# Feature importance:\n",
    "# How important each feature is for making a prediction \n",
    "\n",
    "feat_importance = clf.feature_importances_\n",
    "feat_names = ['Linearity', 'Planarity', 'Scattering', 'Omnivariance', 'Anisotropy',\n",
    "              'Eigentropy', 'Sum of eigenvalues','Change of curvature', 'Local density', 'Relative height', 'Verticality']\n",
    "\n",
    "print \"FEATURE IMPORTANCE:\"\n",
    "for count, elem in enumerate(feat_importance):\n",
    "    print feat_names[count], \"%.4f\" % elem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a classifier and then call it later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filename.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'classifier_xxNN.pkl')\n",
    "# Load job:\n",
    "# clf = joblib.load('filename.pkl') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
